## Must-read papers on Machine Reading Comprehension. 

### (Important)Pre-Train Model
1.  **A Neural Probabilistic Language Model.**  XXX. XXX. [paper](http://papers.nips.cc/paper/1839-a-neural-probabilistic-language-model.pdf)

2.  **Efficient Estimation of Word Representations in Vector Space.** XXX. XXX. [paper](https://arxiv.org/abs/1301.3781.pdf).[code](https://github.com/deborausujono/word2vecpy)

3.  **GloVe: Global Vectors for Word Representation** XXX. XXX. [paper](https://nlp.stanford.edu/pubs/glove.pdf). [code](https://github.com/GradySimon/tensorflow-glove)

   4   **Deep contextualized word representations** XXX. XXX. [paper](https://arxiv.org/pdf/1802.05365.pdf). [code](https://github.com/Rokid/ELMo-chinese)

5.  **Attention Is All You Need** XXX.XXX. [paper](https://arxiv.org/pdf/1706.03762.pdf). [code](https://github.com/Kyubyong/transformer)

6.  **Improving Language Understanding by Generative Pre-Training** XXX. XXX. [paper](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf). [code](https://github.com/openai/finetune-transformer-lm)

7. **Language Models are Unsupervised Multitask Learners** XXX. XXX. [paper](https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf). [code](https://github.com/openai/gpt-2)

8.  **BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding** XXX. XXX. [paper](https://arxiv.org/pdf/1810.04805.pdf). [code](https://github.com/google-research/bert). [application](https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650763568&idx=4&sn=f595b6783d9adf597273747acdcca910&chksm=871ab54eb06d3c58285276cc4c9cc8525283050623e93c1803974918a40654bbf19c9314ce29&mpshare=1&scene=23&srcid=#rd)

### (Important)Attention
1. **NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE** XXX. XXX. [paper](https://arxiv.org/pdf/1409.0473.pdf)
2. **Effective Approaches to Attention-based Neural Machine Translation** XXX. XXX [paper](https://arxiv.org/pdf/1508.04025.pdf)
3. **Teaching Machines to Read and Comprehend.** XXX. XXX. [paper](https://papers.nips.cc/paper/5945-teaching-machines-to-read-and-comprehend.pdf)
4. **Long Short-Term Memory-Networks for Machine Reading.** XXX. XXX [paper](https://aclweb.org/anthology/D16-1053)
5. **FEED-FORWARD NETWORKS WITH ATTENTION CAN SOLVE SOME LONG-TERM MEMORY PROBLEMS** XXX. XXX. [paper](https://arxiv.org/pdf/1512.08756.pdf)
6. **A STRUCTURED SELF-ATTENTIVE SENTENCE EMBEDDING** XXX. XXX. [paper](https://arxiv.org/pdf/1703.03130.pdf)
7. **FRUSTRATINGLY SHORT ATTENTION SPANS IN NEURAL LANGUAGE MODELING**. XXX. XXX. [paper](https://arxiv.org/pdf/1702.04521.pdf)


### Model Architecture
1.   **Memory networks.** Jason Weston, Sumit Chopra, and Antoine Bordes. arXiv preprint arXiv:1410.3916 (2014). [paper](https://arxiv.org/pdf/1410.3916)
2. **Teaching Machines to Read and Comprehend.** Karl Moritz Hermann, Tomáš Kočiský, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom. NIPS 2015. [paper](https://papers.nips.cc/paper/5945-teaching-machines-to-read-and-comprehend.pdf).[code](https://github.com/18140663659/DeepMind-Teaching-Machines-to-Read-and-Comprehend)
3.  **Text Understanding with the Attention Sum Reader Network.** Rudolf Kadlec, Martin Schmid, Ondrej Bajgar, and Jan Kleindienst. ACL 2016. [paper](http://www.aclweb.org/anthology/P16-1086).[code](https://github.com/rkadlec/asreader)
4. **A Thorough Examination of the Cnn/Daily Mail Reading Comprehension Task.**  Danqi Chen, Jason Bolton, and Christopher D. Manning. ACL 2016. [paper](https://www.aclweb.org/anthology/P16-1223).[code](https://github.com/18140663659/CNN-Daily-Mail-Reading-Comprehension-Task)
5. **Long Short-Term Memory-Networks for Machine Reading.** Jianpeng Cheng, Li Dong, and Mirella Lapata. EMNLP 2016. [paper](https://aclweb.org/anthology/D16-1053).[code](https://github.com/egeersu/LSTMN)
6.  **Key-value Memory Networks for Directly Reading Documents.** Alexander  Miller, Adam Fisch, Jesse Dodge, Amir-Hossein Karimi, Antoine Bordes, and Jason Weston. EMNLP 2016. [paper](http://www.aclweb.org/anthology/D16-1147).[code](https://github.com/lc222/key-value-MemNN)
7. **Modeling Human Reading with Neural Attention.** Michael Hahn and Frank Keller. EMNLP 2016. [paper](http://www.aclweb.org/anthology/D16-1009) 
8. **Learning Recurrent Span Representations for Extractive Question Answering** Kenton Lee, Shimi Salant, Tom Kwiatkowski, Ankur Parikh, Dipanjan Das, and Jonathan Berant. arXiv preprint arXiv:1611.01436 (2016). [paper](https://arxiv.org/pdf/1611.01436).[code](https://github.com/shimisalant/RaSoR)
9. **Multi-Perspective Context Matching for Machine Comprehension.** Zhiguo Wang, Haitao Mi, Wael Hamza, and Radu Florian. arXiv preprint arXiv:1612.04211. [paper](https://arxiv.org/pdf/1612.04211)。[code](https://github.com/QiujiangJin/Implementation-of-Multi-Perspective-Context-Matching-for-Machine-Comprehension)
10. **Natural Language Comprehension with the Epireader.** Adam Trischler, Zheng Ye, Xingdi Yuan, and Kaheer Suleman. EMNLP 2016. [paper](https://www.aclweb.org/anthology/D16-1013)
11. **Iterative Alternating Neural Attention for Machine Reading.** Alessandro Sordoni, Philip Bachman, Adam Trischler, and Yoshua Bengio. arXiv preprint arXiv:1606.02245 (2016). [paper](https://arxiv.org/pdf/1606.02245)
12. **(Important)Bidirectional Attention Flow for Machine Comprehension.** Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and Hannaneh Hajishirzi. ICLR 2017. [paper](https://arxiv.org/pdf/1611.01603.pdf).[code](https://github.com/jojonki/BiDAF)
13. **(Important)Machine Comprehension Using Match-lstm and Answer Pointer.** Shuohang Wang and Jing Jiang. arXiv preprint arXiv:1608.07905 (2016). [paper](https://arxiv.org/pdf/1608.07905).[code](https://github.com/18140663659/QA-matchLSTM)
14. **Gated Self-matching Networks for Reading Comprehension and Question Answering.** Wenhui Wang, Nan Yang, Furu Wei, Baobao Chang, and Ming Zhou.  ACL 2017. [paper](http://www.aclweb.org/anthology/P17-1018).[code](https://github.com/alexduer/squad-gated-rep)
15. **Attention-over-attention Neural Networks for Reading Comprehension.**  Yiming Cui, Zhipeng Chen, Si Wei, Shijin Wang, Ting Liu, and Guoping Hu. ACL 2017. [paper](http://aclweb.org/anthology/P17-1055).[code](https://github.com/OlavHN/attention-over-attention)
16. **Gated-attention Readers for Text Comprehension.** Bhuwan Dhingra, Hanxiao Liu, Zhilin Yang, William W. Cohen, and Ruslan Salakhutdinov. ACL 2017. [paper](http://aclweb.org/anthology/P17-1168).[code](https://github.com/18140663659/ga-reader)
17. **A Constituent-Centric Neural Architecture for Reading Comprehension.** Pengtao Xie and Eric Xing. ACL 2017. [paper](http://aclweb.org/anthology/P17-1129) .[code](https://github.com/haoransh/Constituent-Centric-Neural-Architecture-for-Reading-Comprehension)
18. **Structural Embedding of Syntactic Trees for Machine Comprehension.**  Rui Liu, Junjie Hu, Wei Wei, Zi Yang, and Eric Nyberg. EMNLP 2017. [paper](http://aclweb.org/anthology/D17-1085)
19. **Accurate Supervised and Semi-Supervised Machine Reading for Long Documents.** Izzeddin Gur, Daniel Hewlett, Alexandre Lacoste, and Llion Jones. EMNLP 2017.  [paper](http://aclweb.org/anthology/D17-1214)
20. **MEMEN: Multi-layer Embedding with Memory Networks for Machine Comprehension.** Boyuan Pan, Hao Li, Zhou Zhao, Bin Cao, Deng Cai, and Xiaofei He. arXiv preprint arXiv:1707.09098 (2017). [paper](https://arxiv.org/pdf/1707.09098)
21. **Dynamic Coattention Networks For Question Answering.** Caiming Xiong, Victor Zhong, and Richard Socher. ICLR 2017 [paper](https://arxiv.org/pdf/1611.01604.pdf).[code](https://github.com/marshmelloX/dynamic-coattention-network)
22. **(Important)R-NET: Machine Reading Comprehension with Self-matching Networks.** Natural Language Computing Group, Microsoft Research Asia. [paper](https://www.microsoft.com/en-us/research/wp-content/uploads/2017/05/r-net.pdf).[code](https://github.com/18140663659/R-Net)
23. **Reasonet: Learning to Stop Reading in Machine Comprehension.** Yelong Shen, Po-Sen Huang, Jianfeng Gao, and Weizhu Chen. KDD 2017. [paper](https://arxiv.org/pdf/1609.05284).[code](https://github.com/AnatoliiPotapov/tf_Reasonet)
24. **FusionNet: Fusing via Fully-Aware Attention with Application to Machine Comprehension.** Hsin-Yuan Huang, Chenguang Zhu, Yelong Shen, and Weizhu Chen.  ICLR 2018. [paper](https://arxiv.org/pdf/1711.07341).[code](https://github.com/theSage21/FusionNet)
25. **Making Neural QA as Simple as Possible but not Simpler.** Dirk Weissenborn, Georg Wiese, and Laura Seiffe. CoNLL 2017. [paper](http://www.aclweb.org/anthology/K17-1028)
26. **Efficient and Robust Question Answering from Minimal Context over Documents.** Sewon Min, Victor Zhong, Richard Socher, and Caiming Xiong. ACL 2018. [paper](http://aclweb.org/anthology/P18-1160)
27. **Simple and Effective Multi-Paragraph Reading Comprehension.** Christopher Clark and Matt Gardner. ACL 2018. [paper](http://aclweb.org/anthology/P18-1078)
28. **Neural Speed Reading via Skim-RNN.** Minjoon Seo, Sewon Min, Ali Farhadi,  and Hannaneh Hajishirzi. ICLR2018. [paper](https://arxiv.org/pdf/1711.02085)
29. **Hierarchical Attention Flow for Multiple-Choice Reading Comprehension.** Haichao Zhu, Furu Wei, Bing Qin, and Ting Liu. AAAI 2018. [paper](https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/viewFile/16331/16177).[code](https://github.com/caldreaming/Hierarchical_Attention_Flow_Model)
30. **Towards Reading Comprehension for Long Documents.** Yuanxing Zhang, Yangbin Zhang, Kaigui Bian, and Xiaoming Li. IJCAI 2018. [paper](https://www.ijcai.org/proceedings/2018/0638.pdf)
31. **Joint Training of Candidate Extraction and Answer Selection for Reading Comprehension.** Zhen Wang, Jiachen Liu, Xinyan Xiao, Yajuan Lyu, and Tian Wu. ACL 2018. [paper](http://aclweb.org/anthology/P18-1159)
32. **(Important)Multi-Passage Machine Reading Comprehension with Cross-Passage Answer Verification.** Yizhong Wang, Kai Liu, Jing Liu, Wei He, Yajuan Lyu, Hua Wu, Sujian Li, and Haifeng Wang. ACL 2018. [paper](http://aclweb.org/anthology/P18-1178)
33. **Reinforced Mnemonic Reader for Machine Reading Comprehension.** Minghao Hu, Yuxing Peng, Zhen Huang, Xipeng Qiu, Furu Wei, and Ming Zhou. IJCAI 2018. [paper](https://www.ijcai.org/proceedings/2018/0570.pdf).[code](https://github.com/ewrfcas/Reinforced-Mnemonic-Reader)
34. **Stochastic Answer Networks for Machine Reading Comprehension.** Xiaodong Liu, Yelong Shen, Kevin Duh, and Jianfeng Gao. ACL 2018. [paper](http://aclweb.org/anthology/P18-1157).[code](https://github.com/kevinduh/san_mrc)
35. **Multi-Granularity Hierarchical Attention Fusion Networks for Reading Comprehension and Question Answering.** Wei Wang, Ming Yan, and Chen Wu. ACL 2018. [paper](http://aclweb.org/anthology/P18-1158). [code](https://github.com/RahulSChand/Multi-Granularity-Hierarchical-Attention-Fusion-Networks-for-Question-Answering---TensorFlow)
36. **A Multi-Stage Memory Augmented Neural Networkfor Machine Reading Comprehension.** Seunghak Yu, Sathish Indurthi, Seohyun Back, and Haejun Lee. ACL 2018 workshop. [paper](http://aclweb.org/anthology/W18-2603)
37. **(Important)S-NET: From Answer Extraction to Answer Generation for Machine Reading Comprehension.** Chuanqi Tan, Furu Wei, Nan Yang, Bowen Du, Weifeng Lv, and  Ming Zhou. AAAI2018. [paper](https://arxiv.org/abs/1706.04815)
38. **Ask the Right Questions: Active Question Reformulation with Reinforcement Learning.** Christian Buck, Jannis Bulian, Massimiliano Ciaramita, Wojciech Gajewski, Andrea Gesmundo, Neil Houlsby, and Wei Wang. ICLR2018. [paper](https://arxiv.org/abs/1705.07830)
39. **(Important)QANet: Combining Local Convolution with Global Self-Attention for Reading Comprehension.** Adams Wei Yu, David Dohan, Minh-Thang Luong, Rui Zhao, Kai Chen, Mohammad Norouzi, and Quoc V. Le. ICLR2018. [paper](https://arxiv.org/abs/1804.09541).[code](https://github.com/NLPLearn/QANet)
40. **Read + Verify: Machine Reading Comprehension with Unanswerable Questions.** Furu Wei, Yuxing Peng, Zhen Huang, Nan Yang, and Ming Zhou. AAAI2019. [paper](https://arxiv.org/abs/1808.05759)

41. **(Important)Multi-style Generative Reading Comprehension.** XXX.XXX. [paper](https://arxiv.org/pdf/1901.02262.pdf)
42. **(Real-Time Open-Domain Question Answering with Dense-Sparse Phrase Index** XXX. XXX. [paper](https://arxiv.org/pdf/1906.05807.pdf)
43. **Explore, Propose, and Assemble: An Interpretable Model for Multi-Hop Reading Comprehension** . XXX. XXX. [paper](https://arxiv.org/pdf/1906.05210.pdf). [code](https://github.com/jiangycTarheel/EPAr)
44. **(Important)Retrieve, Read, Rerank: Towards End-to-End Multi-Document Reading Comprehension**. XXX. XXX. [paper](https://arxiv.org/pdf/1906.04618.pdf). [code](https://github.com/huminghao16/RE3QA)
45. **A Survey on Neural Machine Reading Comprehension** . XXX. XXX. [paper](https://arxiv.org/pdf/1906.03824.pdf)
46. **RankQA: Neural Question Answering with Answer Re-Ranking**. XXX. XXX. [paper](https://arxiv.org/pdf/1906.03008.pdf). [code](https://github.com/bernhard2202/rankqa)
47. **(Important)Multi-hop Reading Comprehension through Question Decomposition and Rescoring**. XXX. XXX. [paper](https://arxiv.org/pdf/1906.02916.pdf). [code](https://github.com/shmsw25/DecompRC)
48. **(New)NLProlog: Reasoning with Weak Unification for Question Answering in Natural Language**. XXX. XXX. [paper](https://arxiv.org/pdf/1906.06187.pdf). [code](https://github.com/leonweber/nlprolog)
49. **(New)Learning to Ask Unanswerable Questions for Machine Reading Comprehension**. XXX. XXX. [paper](https://arxiv.org/pdf/1906.06045.pdf). 
50. **(New)Multi-Hop Paragraph Retrieval for Open-Domain Question Answering**. XXX. XXX. [paper](https://arxiv.org/pdf/1906.06606.pdf). [code](https://github.com/yairf11/MUPPET)






### Datasets
1. (SQuAD 1.0) **SQuAD: 100,000+ Questions for Machine Comprehension of Text.** Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. EMNLP 2016. [paper](https://aclweb.org/anthology/D16-1264)
2. (SQuAD 2.0) **Know What You Don't Know: Unanswerable Questions for SQuAD.**
Pranav Rajpurkar, Robin Jia, and Percy Liang. ACL 2018. [paper](http://aclweb.org/anthology/P18-2124)
3. (MS MARCO) **MS MARCO: A Human Generated MAchine Reading COmprehension Dataset.** Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, and Li Deng.  arXiv preprint arXiv:1611.09268 (2016). [paper](https://arxiv.org/pdf/1611.09268)
4. (Quasar) **Quasar: Datasets for Question Answering by Search and Reading.** Bhuwan Dhingra, Kathryn Mazaitis, and William W. Cohen. arXiv preprint arXiv:1707.03904 (2017). [paper](https://arxiv.org/pdf/1707.03904)
5. (TriviaQA) **TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension.** Mandar Joshi, Eunsol Choi, Daniel S. Weld, Luke Zettlemoyer. arXiv preprint arXiv:1705.03551 (2017). [paper](https://arxiv.org/pdf/1705.03551)
6. (SearchQA) **SearchQA: A New Q&A Dataset Augmented with Context from a Search Engine.**
Matthew Dunn, Levent Sagun, Mike Higgins, V. Ugur Guney, Volkan Cirik, and Kyunghyun Cho. arXiv preprint arXiv:1704.05179 (2017). [paper](https://arxiv.org/pdf/1704.05179)
7. (QuAC) **QuAC : Question Answering in Context.** Eunsol Choi, He He, Mohit Iyyer, Mark Yatskar, Wen-tau Yih, Yejin Choi, Percy Liang, and  Luke Zettlemoyer. arXiv preprint arXiv:1808.07036 (2018). [paper](https://arxiv.org/pdf/1808.07036)
8. (CoQA) **CoQA: A Conversational Question Answering Challenge.** Siva Reddy, Danqi Chen, and Christopher D. Manning. arXiv preprint arXiv:1808.07042 (2018). [paper](https://arxiv.org/pdf/1808.07042)
7. (MCTest) **MCTest: A Challenge Dataset for the Open-Domain Machine Comprehension of Text.** Matthew Richardson, Christopher J.C. Burges, and Erin Renshaw. EMNLP 2013. [paper](http://www.aclweb.org/anthology/D13-1020).
8. (CNN/Daily Mail) **Teaching Machines to Read and Comprehend.** Hermann, Karl Moritz, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom. NIPS 2015. [paper](https://papers.nips.cc/paper/5945-teaching-machines-to-read-and-comprehend.pdf)
9. (CBT) **The Goldilocks Principle: Reading Children's Books with Explicit Memory Representations.** Felix Hill, Antoine Bordes, Sumit Chopra, and Jason Weston. arXiv preprint arXiv:1511.02301 (2015). [paper](https://arxiv.org/pdf/1511.02301)
10. (bAbi) **Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks.** Jason Weston, Antoine Bordes, Sumit Chopra, Alexander M. Rush, Bart van Merriënboer, Armand Joulin, and Tomas Mikolov. arXiv preprint arXiv:1502.05698 (2015). [paper](https://arxiv.org/pdf/1502.05698)
11. (LAMBADA) **The LAMBADA Dataset:Word Prediction Requiring a Broad Discourse Context.** Denis Paperno, Germ ́an Kruszewski, Angeliki Lazaridou, Quan Ngoc Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fern ́andez. ACL 2016. [paper](https://www.aclweb.org/anthology/P16-1144)
12. (SCT) **LSDSem 2017 Shared Task: The Story Cloze Test.** Nasrin Mostafazadeh, Michael Roth, Annie Louis,Nathanael Chambers, and James F. Allen. ACL 2017 workshop. [paper](http://aclweb.org/anthology/W17-0906)
13. (Who did What) **Who did What: A Large-Scale Person-Centered Cloze Dataset** Takeshi Onishi, Hai Wang, Mohit Bansal, Kevin Gimpel, and David McAllester. EMNLP 2016. [paper](https://aclweb.org/anthology/D16-1241)
14. (NewsQA) **NewsQA: A Machine Comprehension Dataset.** Adam Trischler, Tong Wang, Xingdi Yuan, Justin Harris, Alessandro Sordoni, Philip Bachman, and Kaheer Suleman. arXiv preprint arXiv:1611.09830 (2016). [paper](https://arxiv.org/pdf/1611.09830)
15. (RACE) **RACE: Large-scale ReAding Comprehension Dataset From Examinations.** Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. EMNLP 2017. [paper](http://aclweb.org/anthology/D17-1082)
16. (ARC) **Think you have Solved Question Answering?Try ARC, the AI2 Reasoning Challenge.** Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot,Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. arXiv preprint arXiv:1803.05457 (2018). [paper](https://arxiv.org/pdf/1803.05457)
17. (MCScript) **MCScript: A Novel Dataset for Assessing Machine Comprehension Using Script Knowledge.** Simon Ostermann, Ashutosh Modi, Michael Roth, Stefan Thater, and Manfred Pinkal. arXiv preprint arXiv:1803.05223.  [paper](https://arxiv.org/pdf/1803.05223.pdf)
18. (NarrativeQA) **The NarrativeQA Reading Comprehension Challenge**.
Tomáš Kočiský, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, Gábor Melis, and Edward Grefenstette. TACL 2018. [paper](http://aclweb.org/anthology/Q18-1023)
19. (DuoRC) **DuoRC: Towards Complex Language Understanding with Paraphrased Reading Comprehension.** Amrita Saha, Rahul Aralikatte, Mitesh M. Khapra, and Karthik Sankaranarayanan. ACL 2018. [paper](http://aclweb.org/anthology/P18-1156)
20. (CLOTH) **Large-scale Cloze Test Dataset Created by Teachers.** Qizhe Xie, Guokun Lai, Zihang Dai, and Eduard Hovy. EMNLP 2018. [paper](https://arxiv.org/pdf/1711.03225)
21. (DuReader) **DuReader: a Chinese Machine Reading Comprehension Dataset from
Real-world Applications.** Wei He, Kai Liu, Yajuan Lyu, Shiqi Zhao, Xinyan Xiao, Yuan Liu, Yizhong Wang, Hua Wu, Qiaoqiao She, Xuan Liu, Tian Wu, and Haifeng Wang. ACL 2018 Workshop. [paper](https://arxiv.org/abs/1711.05073)
22. (CliCR) **CliCR: a Dataset of Clinical Case Reports for Machine Reading Comprehension.** Simon Suster and Walter Daelemans. NAACL 2018. [paper](http://aclweb.org/anthology/N18-1140) 
